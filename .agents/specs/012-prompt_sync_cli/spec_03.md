# Sprint Overview

This sprint builds upon spec_01.md and spec_02.md which have already been implemented. Review those spec documents to gain a full context of this development sprint.

## Background

We have a mechanism for syncing and loading prompts from LangSmith (implemented in previous sprints). Now we need to integrate these synced prompts into our workflow code, replacing hardcoded prompt strings with dynamically loaded prompts from LangSmith.

## Goals

### 1. Refactor Workflow Files to Use Synced Prompts

Replace hardcoded prompts in three workflow files with loaded prompts from LangSmith:

- `app/services/job_intake_service/workflows/gap_analysis.py` → Use `PromptName.GAP_ANALYSIS`
- `app/services/job_intake_service/workflows/stakeholder_analysis.py` → Use `PromptName.STAKEHOLDER_ANALYSIS`
- `app/services/job_intake_service/workflows/resume_refinement.py` → Use `PromptName.RESUME_ALIGNMENT_WORKFLOW`

**Approach:**

- Remove all hardcoded `_SYSTEM_PROMPT` and `_USER_PROMPT` constants
- Load prompts at module level using `get_prompt(PromptName.XXX)`
- Replace `ChatPromptTemplate.from_messages(...)` with the loaded prompt
- Use direct prompt replacement: `_prompt, _ = get_prompt(PromptName.XXX)` then `_chain = _prompt | _llm | StrOutputParser()`
- Fail fast with clear error messages if prompts are missing (will propagate from underlying `load_prompt()`)
- Resume refinement workflow: Keep tools and all other logic intact; only replace the prompt loading

**Note:** The `extract_experience_updates.json` prompt exists but has no corresponding workflow yet, so it's excluded from this sprint.

### 2. Generate TypedDict Classes and get_prompt Function

Auto-generate TypedDict classes for prompt input variables and a type-safe `get_prompt()` function that returns both the prompt template and its input TypedDict type.

**Requirements:**

- Generate TypedDict definitions for **all** synced prompts (not just ones with workflows)
- Store in auto-generated file at `src/core/prompts/input_types.py`
- Include header comments indicating file is auto-generated and should not be manually edited
- Generate during `prompts sync` command, called immediately after `generate_prompt_enum()`
- Extract input variables from loaded prompt's `input_variables` attribute
- Type all input variables as `str`
- If prompt has no input variables, generate empty TypedDict with `pass` statement
- Include a `get_prompt()` function with type-safe overloads that returns tuple of (prompt_template, input_type)

**Naming Convention:**

- Pattern: `[PromptName]Input`
- Examples: `GapAnalysisInput`, `StakeholderAnalysisInput`, `ResumeAlignmentWorkflowInput`, `ExtractExperienceUpdatesInput`

**Implementation:**

- Create new module: `src/core/prompts/input_types_generator.py`
- Function: `generate_prompt_input_types()`
- Called from `src/core/prompts/cli.py` immediately after `generate_prompt_enum()`
- Loads each prompt using `load_prompt()` to extract `input_variables` attribute

**Example Generated File (`src/core/prompts/input_types.py`):**

```python
# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# Generated by: prompts sync
# To update: run `python cli.py prompts sync`

from __future__ import annotations

from typing import Literal, TypedDict, overload

from langchain_core.prompts.chat import ChatPromptTemplate
from langchain_core.runnables.base import RunnableSequence

from .loader import load_prompt
from .names import PromptName


class GapAnalysisInput(TypedDict):
    """Input variables for gap_analysis prompt template."""

    job_description: str
    work_experience: str


class StakeholderAnalysisInput(TypedDict):
    """Input variables for stakeholder_analysis prompt template."""

    job_description: str
    work_experience: str


class ResumeAlignmentWorkflowInput(TypedDict):
    """Input variables for resume_alignment_workflow prompt template."""

    job_description: str
    current_resume: str
    gap_analysis: str
    stakeholder_analysis: str
    work_experience: str


class ExtractExperienceUpdatesInput(TypedDict):
    """Input variables for extract_experience_updates prompt template."""

    pass  # No input variables for this prompt template


# Type-safe get_prompt function with overloads for each prompt template
# Each overload uses the specific concrete type of the prompt template (detected at generation time)
@overload
def get_prompt(name: Literal[PromptName.GAP_ANALYSIS]) -> tuple[ChatPromptTemplate, type[GapAnalysisInput]]: ...

@overload
def get_prompt(name: Literal[PromptName.STAKEHOLDER_ANALYSIS]) -> tuple[ChatPromptTemplate, type[StakeholderAnalysisInput]]: ...

@overload
def get_prompt(name: Literal[PromptName.RESUME_ALIGNMENT_WORKFLOW]) -> tuple[RunnableSequence, type[ResumeAlignmentWorkflowInput]]: ...

@overload
def get_prompt(name: Literal[PromptName.EXTRACT_EXPERIENCE_UPDATES]) -> tuple[ChatPromptTemplate, type[ExtractExperienceUpdatesInput]]: ...


def get_prompt(name: PromptName) -> tuple[ChatPromptTemplate | RunnableSequence, type[TypedDict]]:
    """Load a prompt template and return it with its input TypedDict type.

    This function combines prompt template loading and type retrieval into a single
    type-safe call. The overloads ensure the correct TypedDict type is
    returned for each prompt template.

    Args:
        name: The PromptName enum member

    Returns:
        Tuple of (loaded prompt template, TypedDict class for inputs)

    Example:
        prompt, InputType = get_prompt(PromptName.GAP_ANALYSIS)
        chain = prompt | llm | StrOutputParser()

        inputs: InputType = {
            "job_description": "...",
            "work_experience": "..."
        }
        result = chain.invoke(inputs)
    """
    prompt = load_prompt(name)

    type_mapping: dict[PromptName, type[TypedDict]] = {
        PromptName.GAP_ANALYSIS: GapAnalysisInput,
        PromptName.STAKEHOLDER_ANALYSIS: StakeholderAnalysisInput,
        PromptName.RESUME_ALIGNMENT_WORKFLOW: ResumeAlignmentWorkflowInput,
        PromptName.EXTRACT_EXPERIENCE_UPDATES: ExtractExperienceUpdatesInput,
    }

    return prompt, type_mapping[name]
```

### 3. Usage Pattern for Type-Safe Prompt Templates

Use the generated `get_prompt()` function to load prompt templates with their input types in a single type-safe call.

**Workflow Integration Pattern:**

```python
from langchain_core.output_parsers import StrOutputParser

from src.core.models import OpenAIModels, get_model
from src.core.prompts.input_types import get_prompt
from src.core.prompts.names import PromptName

# Load prompt and input type at module level
_prompt, _InputType = get_prompt(PromptName.GAP_ANALYSIS)
_llm = get_model(OpenAIModels.gpt_4o)
_chain = _prompt | _llm | StrOutputParser()


def analyze_job_experience_fit(job_description: str, experiences: list[Experience]) -> str:
    """Analyze how well user experiences match a job description."""
    experience_summary = _format_experience_for_analysis(experiences)

    # Type the input dict - type checker validates structure
    inputs: _InputType = {
        "job_description": job_description,
        "work_experience": experience_summary
    }

    return _chain.invoke(inputs)
```

**Alternative: Import specific TypedDict class for clarity:**

```python
from src.core.prompts.input_types import GapAnalysisInput, get_prompt
from src.core.prompts.names import PromptName

_prompt, _ = get_prompt(PromptName.GAP_ANALYSIS)
_chain = _prompt | _llm | StrOutputParser()

def analyze_job_experience_fit(job_description: str, experiences: list[Experience]) -> str:
    inputs: GapAnalysisInput = {  # Explicit type name
        "job_description": job_description,
        "work_experience": experience_summary
    }
    return _chain.invoke(inputs)
```

**Benefits:**

- Single function call returns both prompt and type
- Type checkers (mypy, pyright) validate all required keys are present
- IDE autocomplete shows available input variable names
- Typos in key names caught at type-check time
- Simple, standard Python typing pattern
- No changes to existing LangChain chain patterns

## Implementation Details

### Files to Create

1. **`src/core/prompts/input_types_generator.py`**

   - Module containing `generate_prompt_input_types()` function
   - **Returns:** `int` (count of TypedDict classes generated)
   - Scans `/prompts` directory for JSON files
   - For each prompt template:
     - Loads using `load_prompt(PromptName.XXX)` to extract `input_variables`
     - Inspects the actual runtime type using `type(prompt)`
     - Determines the fully qualified type name (e.g., `ChatPromptTemplate`, `RunnableSequence`)
     - Maps type to correct import path (e.g., `langchain_core.prompts.chat.ChatPromptTemplate`)
     - Validates the type is recognized (raises error if not)
   - Generates TypedDict classes with appropriate naming
   - Generates imports for all detected prompt template types
   - Generates `get_prompt()` function with type-safe overloads where each overload uses the **specific concrete type** detected for that prompt template
   - Implementation function signature uses dynamically built union of all detected types
   - Writes to `src/core/prompts/input_types.py`

   **TypedDict Class Name Generation:**

   ```python
   # Convert enum value to TypedDict class name
   # Example: "gap_analysis" -> "GapAnalysis" -> "GapAnalysisInput"
   words = prompt_name.value.split("_")
   class_name = "".join(word.capitalize() for word in words) + "Input"
   ```

   **Type Detection Logic:**

   - Load prompt template and get its type: `prompt_type = type(prompt)`
   - Get type name: `prompt_type.__name__` (e.g., `"ChatPromptTemplate"`)
   - Get module path: `prompt_type.__module__` (e.g., `"langchain_core.prompts.chat"`)
   - Validate type is recognized (expected types: `ChatPromptTemplate`, `RunnableSequence`)
   - If unrecognized type, raise error with full type info for debugging
   - Collect all unique types across all prompt templates
   - Generate union type for implementation function signature: `Type1 | Type2 | ...`
   - Generate appropriate imports and use specific type in each overload

2. **`src/core/prompts/input_types.py`** (auto-generated)
   - Created by running `python cli.py prompts sync`
   - Contains TypedDict classes for all synced prompt templates
   - Contains `get_prompt()` function with overloads
   - **Committed to git** so workflows can import on fresh clones
   - If file doesn't exist when generator runs, create with empty placeholder and notify user to re-run command

### Files to Modify

1. **`src/core/prompts/__init__.py`**

   - Add exports for `get_prompt` from `.input_types`
   - Keep existing exports for `load_prompt` and `PromptName`
   - Final exports: `load_prompt`, `PromptName`, `get_prompt`

2. **`src/core/prompts/cli.py`**

   - Import `generate_prompt_input_types` from `input_types_generator`
   - Call `generate_prompt_input_types()` immediately after `generate_prompt_enum()`
   - Display console message about input types generation

3. **`app/services/job_intake_service/workflows/gap_analysis.py`**

   - Remove the `_SYSTEM_PROMPT` constant (large multi-line string defining the system prompt template)
   - Remove the `_USER_PROMPT` constant (multi-line string defining the user prompt template)
   - Replace with: `_prompt, _ = get_prompt(PromptName.GAP_ANALYSIS)`
   - Update chain definition from `ChatPromptTemplate.from_messages([...])` to just `_prompt`
   - Full chain: `_chain = _prompt | _llm | StrOutputParser()`
   - Update imports: `from src.core.prompts import get_prompt, PromptName`
   - Add input type annotation in `analyze_job_experience_fit` function for type safety

4. **`app/services/job_intake_service/workflows/stakeholder_analysis.py`**

   - Remove the `_SYSTEM_PROMPT` constant (multi-line string defining the system prompt template)
   - Remove the `_USER_PROMPT` constant (multi-line string defining the user prompt template)
   - Replace with: `_prompt, _ = get_prompt(PromptName.STAKEHOLDER_ANALYSIS)`
   - Update chain definition from `ChatPromptTemplate.from_messages([...])` to just `_prompt`
   - Full chain: `_chain = _prompt | _llm | StrOutputParser()`
   - Update imports: `from src.core.prompts import get_prompt, PromptName`
   - Add input type annotation in `analyze_stakeholders` function for type safety

5. **`app/services/job_intake_service/workflows/resume_refinement.py`**
   - Remove the `SYSTEM_PROMPT_TEMPLATE` constant (multi-line string)
   - Replace prompt creation with: `_prompt, _ = get_prompt(PromptName.RESUME_ALIGNMENT_WORKFLOW)`
   - Update chain definition to use `_prompt` instead of `ChatPromptTemplate.from_messages([("system", SYSTEM_PROMPT_TEMPLATE), ("placeholder", "{message_history}")])`
   - **Assumption:** The synced `resume_alignment_workflow` prompt template from LangSmith already includes the message_history placeholder structure
   - Keep all tool definitions (`propose_resume_draft`) and other logic unchanged
   - Update imports: `from src.core.prompts import get_prompt, PromptName`

### CLI Integration

In `src/core/prompts/cli.py`, update the `sync_prompts_command` function:

```python
@prompts_app.command("sync")
def sync_prompts_command(
    fail_fast: bool = typer.Option(True, "--fail-fast/--no-fail-fast", ...),
) -> None:
    """Sync prompts from LangSmith to local storage."""
    # ... existing sync logic ...

    # Generate enum
    console.print("Generating PromptName enum...", style="cyan")
    enum_count = generate_prompt_enum()
    console.print(f"Generated enum with {enum_count} prompts", style="green")

    # NEW: Generate input types
    console.print("Generating prompt input types...", style="cyan")
    from .input_types_generator import generate_prompt_input_types
    types_count = generate_prompt_input_types()
    console.print(f"Generated {types_count} input type classes", style="green")
```

### Error Handling

**For missing input_types.py on first run:**

- If `input_types.py` doesn't exist when generator runs:
  - Create empty placeholder file with stub content
  - Log message: "Created placeholder input_types.py - please re-run sync command"
  - Continue with sync process
- On second run, generator will properly populate the file

**For missing prompt templates during workflow import:**

- Let `FileNotFoundError` propagate naturally from `load_prompt()`
- Error message from `load_prompt()` should direct users to run `python cli.py prompts sync`
- No try/catch blocks needed in workflow files

**For generation errors:**

- If prompt template loading fails during generation, log error and continue with other prompt templates
- Report count of successful generations vs failures
- Empty prompt templates (no input variables) generate TypedDict with `pass` statement

**For unrecognized prompt template types:**

- If `type(prompt).__name__` is not `ChatPromptTemplate` or `RunnableSequence`:
  - Raise error with full type information: `f"Unrecognized prompt template type: {type(prompt)} (module: {type(prompt).__module__}, name: {type(prompt).__name__})"`
  - This allows developers to inspect logs and extend generator to support new types
  - Do not continue with that prompt template - fail fast for type issues

## Technical Decisions

- **No testing in this sprint:** Tests for prompt template loading already exist from previous sprint
- **No fallback to hardcoded prompt templates:** Remove all hardcoded prompt templates entirely; fail fast if synced prompt templates missing
- **No migration documentation needed:** This spec serves as the documentation for what changed and why
- **Module-level loading:** Load prompt templates once at module initialization for performance
- **Direct prompt template replacement:** Loaded prompt templates are complete `ChatPromptTemplate` objects that can replace hardcoded templates directly
- **Generate types for all prompt templates:** Even if prompt template has no workflow, generate TypedDict for future use
- **All input variables typed as str:** Simplifies generation logic; sufficient for current use cases
- **Specific concrete types in overloads:** Each `get_prompt()` overload uses the specific runtime type of that prompt template (e.g., `ChatPromptTemplate`, `RunnableSequence`) rather than a union type, providing maximum type safety and accuracy
- **Dynamic union type generation:** The implementation function signature's return type is dynamically built from the union of all detected prompt template types (e.g., if only `ChatPromptTemplate` is found, use that; if both `ChatPromptTemplate` and `RunnableSequence` are found, use `ChatPromptTemplate | RunnableSequence`)
- **Commit generated files:** Both `names.py` and `input_types.py` are committed to git so fresh clones can immediately import and use workflows
- **Re-export from **init**.py:** Export `get_prompt` and `PromptName` from `src.core.prompts/__init__.py` for convenient imports
- **Terminology:** Standardize on "prompt template" throughout codebase and documentation

## Success Criteria

Sprint is complete when:

1. **Generation works:** Running `python cli.py prompts sync` successfully generates both `names.py` and `input_types.py`
2. **All workflows refactored:** Three workflow files (`gap_analysis.py`, `stakeholder_analysis.py`, `resume_refinement.py`) successfully load prompt templates using `get_prompt()`
3. **App runs without errors:** Application starts and workflows execute without import errors or runtime errors
4. **Type checking passes:** Running `mypy` or `pyright` on workflow files shows no type errors related to prompt template usage
5. **Generated files committed:** Both `names.py` and `input_types.py` are committed to git and can be imported on fresh clones
