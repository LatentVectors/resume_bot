# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# Generated by: python -m tools.cli prompts sync
# To update: run `python -m tools.cli prompts sync`

from __future__ import annotations

from typing import Literal, overload

from langchain_core.prompts.chat import ChatPromptTemplate

from .prompts import PromptName, load_prompt


@overload
def get_prompt(name: Literal[PromptName.GAP_ANALYSIS]) -> ChatPromptTemplate: ...

@overload
def get_prompt(name: Literal[PromptName.RESUME_ALIGNMENT_WORKFLOW]) -> ChatPromptTemplate: ...

@overload
def get_prompt(name: Literal[PromptName.STAKEHOLDER_ANALYSIS]) -> ChatPromptTemplate: ...


def get_prompt(name: PromptName) -> ChatPromptTemplate:
    """Load a prompt template by name.

    This function provides type-safe prompt loading with overloads that return
    the specific concrete type for each prompt template.

    Args:
        name: The PromptName enum member

    Returns:
        The loaded prompt template (ChatPromptTemplate or RunnableSequence)

    Example:
        from src.shared import PromptName
        from src.shared.get_prompt import get_prompt
        from src.shared.prompt_types import GapAnalysisInput

        prompt = get_prompt(PromptName.GAP_ANALYSIS)
        chain = prompt | llm | StrOutputParser()

        inputs: GapAnalysisInput = {
            "job_description": "...",
            "work_experience": "..."
        }
        result = chain.invoke(inputs)
    """
    return load_prompt(name)
