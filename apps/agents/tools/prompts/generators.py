"""Code generators for prompt-related types and functions.

This module generates:
- PromptName enum (src/shared/prompt_names.py)
- TypedDict classes for prompt input types (src/shared/prompt_types.py)
- Type-safe get_prompt function with overloads (src/shared/get_prompt.py)
"""

from __future__ import annotations

import json

from .constants import PROMPTS_DIR, SRC_DIR


# ==================== Enum Generator ====================


def generate_prompt_enum() -> int:
    """Generate the PromptName enum as a standalone file.

    Creates src/shared/prompt_names.py with the PromptName enum based on
    the JSON files in the prompts directory.

    Returns:
        Number of prompt enum members generated
    """
    # Scan prompts directory
    prompt_files = sorted(PROMPTS_DIR.glob("*.json"))

    # Generate enum members (create empty enum if no prompts found)
    enum_members = []
    for prompt_file in prompt_files:
        # Convert filename to enum member name (uppercase with underscores)
        name = prompt_file.stem  # Remove .json extension
        enum_name = name.upper()
        enum_members.append(f'    {enum_name} = "{name}"')

    # Build the enum body
    if enum_members:
        enum_body = "\n".join(enum_members)
    else:
        enum_body = "    pass"

    # Generate the complete file
    content = f'''# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# Generated by: python -m tools prompts sync
# To update: run `python -m tools prompts sync`

from enum import Enum


class PromptName(Enum):
    """Enum of available prompts in the /prompts directory."""

{enum_body}
'''

    # Write to file
    output_file = SRC_DIR / "shared" / "prompt_names.py"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(content, encoding="utf-8")

    return len(enum_members)


# ==================== Input Types Generator ====================


def generate_prompt_input_types() -> int:
    """Generate TypedDict classes for all prompt templates.

    Scans the prompts directory, loads each prompt template, extracts input variables,
    and generates TypedDict classes in src/shared/prompt_types.py.

    Returns:
        Number of TypedDict classes generated
    """
    from langchain_core.load.load import loads

    # Scan prompts directory
    prompt_files = sorted(PROMPTS_DIR.glob("*.json"))

    if not prompt_files:
        return 0

    # Collect data for each prompt template
    prompt_data: list[dict] = []
    has_message_placeholders = False

    for prompt_file in prompt_files:
        prompt_name_str = prompt_file.stem
        try:
            # Load the prompt JSON
            with prompt_file.open() as f:
                data = json.load(f)

            # Get the committed prompt manifest
            if "committed_prompt" not in data:
                continue
            manifest = data["committed_prompt"]

            # Deserialize with LangChain's loads
            prompt = loads(json.dumps(manifest))

            # Get input variables
            input_vars = getattr(prompt, "input_variables", [])

            # Detect MessagesPlaceholder variables
            message_placeholder_vars = set()
            if hasattr(prompt, "messages"):
                for msg in prompt.messages:
                    if type(msg).__name__ == "MessagesPlaceholder":
                        message_placeholder_vars.add(msg.variable_name)
                        has_message_placeholders = True

            # Generate TypedDict class name
            words = prompt_name_str.split("_")
            class_name = "".join(word.capitalize() for word in words) + "Input"

            prompt_data.append(
                {
                    "enum_value": prompt_name_str,
                    "class_name": class_name,
                    "input_vars": input_vars,
                    "message_placeholder_vars": message_placeholder_vars,
                }
            )

        except Exception:
            continue

    if not prompt_data:
        return 0

    # Generate the file content
    content = _generate_input_types_file(prompt_data, has_message_placeholders)

    # Write to file
    output_file = SRC_DIR / "shared" / "prompt_types.py"
    output_file.write_text(content, encoding="utf-8")

    return len(prompt_data)


def _generate_input_types_file(prompt_data: list[dict], has_message_placeholders: bool) -> str:
    """Generate the complete input_types.py file content.

    Args:
        prompt_data: List of dicts with prompt metadata
        has_message_placeholders: Whether any prompt uses MessagesPlaceholder

    Returns:
        Complete file content as string
    """
    # Generate TypedDict classes
    typeddict_classes = []
    for data in prompt_data:
        class_name = data["class_name"]
        input_vars = data["input_vars"]
        message_placeholder_vars = data.get("message_placeholder_vars", set())

        if input_vars:
            # Generate fields with appropriate types
            fields = []
            for var in input_vars:
                if var in message_placeholder_vars:
                    fields.append(f"    {var}: Sequence[BaseMessage]")
                else:
                    fields.append(f"    {var}: str")
            fields_str = "\n".join(fields)

            typeddict_class = f'''class {class_name}(TypedDict):
    """Input variables for {data["enum_value"]} prompt template."""

{fields_str}
'''
        else:
            typeddict_class = f'''class {class_name}(TypedDict):
    """Input variables for {data["enum_value"]} prompt template."""

    pass  # No input variables for this prompt template
'''
        typeddict_classes.append(typeddict_class)

    typeddict_classes_str = "\n\n".join(typeddict_classes)

    # Generate imports based on whether message placeholders are used
    if has_message_placeholders:
        imports = """from __future__ import annotations

from collections.abc import Sequence
from typing import TypedDict

from langchain_core.messages import BaseMessage"""
    else:
        imports = """from __future__ import annotations

from typing import TypedDict"""

    # Assemble complete file
    content = f"""# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# Generated by: python -m tools.cli prompts sync
# To update: run `python -m tools.cli prompts sync`

{imports}


{typeddict_classes_str}
"""

    return content


# ==================== Get Prompt Generator ====================


def generate_get_prompt() -> int:
    """Generate the get_prompt.py file with type-safe overloads.

    Scans the prompts directory, loads each prompt template to detect its runtime type,
    and generates a get_prompt() function with overloads that return the specific type.
    Output is written to src/shared/get_prompt.py.

    Returns:
        Number of overloads generated
    """
    from langchain_core.load.load import loads

    # Scan prompts directory
    prompt_files = sorted(PROMPTS_DIR.glob("*.json"))

    if not prompt_files:
        return 0

    # Collect data for each prompt template
    prompt_data: list[dict] = []
    type_names_seen: set[str] = set()

    for prompt_file in prompt_files:
        prompt_name_str = prompt_file.stem
        try:
            # Load the prompt JSON
            with prompt_file.open() as f:
                data = json.load(f)

            # Get the committed prompt manifest
            if "committed_prompt" not in data:
                continue
            manifest = data["committed_prompt"]

            # Deserialize with LangChain's loads
            prompt = loads(json.dumps(manifest))

            # Detect the runtime type
            prompt_type = type(prompt)
            type_name = prompt_type.__name__

            # Validate type is recognized
            if type_name not in ("ChatPromptTemplate", "RunnableSequence"):
                continue

            # Track unique types
            type_names_seen.add(type_name)

            prompt_data.append(
                {
                    "enum_name": prompt_name_str.upper(),
                    "type_name": type_name,
                }
            )

        except Exception:
            continue

    if not prompt_data:
        return 0

    # Generate the file content
    content = _generate_get_prompt_file(prompt_data, type_names_seen)

    # Write to file
    output_file = SRC_DIR / "shared" / "get_prompt.py"
    output_file.write_text(content, encoding="utf-8")

    return len(prompt_data)


def _generate_get_prompt_file(prompt_data: list[dict], type_names_seen: set[str]) -> str:
    """Generate the complete get_prompt.py file content.

    Args:
        prompt_data: List of dicts with prompt metadata
        type_names_seen: Set of unique prompt template type names

    Returns:
        Complete file content as string
    """
    # Build imports based on detected types
    type_imports = []
    if "ChatPromptTemplate" in type_names_seen:
        type_imports.append("from langchain_core.prompts.chat import ChatPromptTemplate")
    if "RunnableSequence" in type_names_seen:
        type_imports.append("from langchain_core.runnables.base import RunnableSequence")

    type_imports_str = "\n".join(type_imports)

    # Build union type for implementation function
    if len(type_names_seen) == 1:
        union_type = list(type_names_seen)[0]
    else:
        union_type = " | ".join(sorted(type_names_seen))

    # Generate overloads
    overloads = []
    for data in prompt_data:
        overload = f'''@overload
def get_prompt(name: Literal[PromptName.{data["enum_name"]}]) -> {data["type_name"]}: ...
'''
        overloads.append(overload)

    overloads_str = "\n".join(overloads)

    # Assemble complete file
    content = f'''# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# Generated by: python -m tools.cli prompts sync
# To update: run `python -m tools.cli prompts sync`

from __future__ import annotations

from typing import Literal, overload

{type_imports_str}

from .prompts import PromptName, load_prompt


{overloads_str}

def get_prompt(name: PromptName) -> {union_type}:
    """Load a prompt template by name.

    This function provides type-safe prompt loading with overloads that return
    the specific concrete type for each prompt template.

    Args:
        name: The PromptName enum member

    Returns:
        The loaded prompt template (ChatPromptTemplate or RunnableSequence)

    Example:
        from src.shared import PromptName
        from src.shared.get_prompt import get_prompt
        from src.shared.prompt_types import GapAnalysisInput

        prompt = get_prompt(PromptName.GAP_ANALYSIS)
        chain = prompt | llm | StrOutputParser()

        inputs: GapAnalysisInput = {{
            "job_description": "...",
            "work_experience": "..."
        }}
        result = chain.invoke(inputs)
    """
    return load_prompt(name)
'''

    return content
