# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# Generated by: prompts sync
# To update: run `python cli.py prompts sync`

from __future__ import annotations

from typing import Literal, overload

from langchain_core.prompts.chat import ChatPromptTemplate

from .loader import load_prompt
from .names import PromptName


@overload
def get_prompt(name: Literal[PromptName.GAP_ANALYSIS]) -> ChatPromptTemplate: ...

@overload
def get_prompt(name: Literal[PromptName.RESUME_ALIGNMENT_WORKFLOW]) -> ChatPromptTemplate: ...

@overload
def get_prompt(name: Literal[PromptName.STAKEHOLDER_ANALYSIS]) -> ChatPromptTemplate: ...


def get_prompt(name: PromptName) -> ChatPromptTemplate:
    """Load a prompt template by name.

    This function provides type-safe prompt loading with overloads that return
    the specific concrete type for each prompt template.

    Args:
        name: The PromptName enum member

    Returns:
        The loaded prompt template (ChatPromptTemplate or RunnableSequence)

    Example:
        from src.core.prompts import PromptName, get_prompt
        from src.core.prompts.input_types import GapAnalysisInput

        prompt = get_prompt(PromptName.GAP_ANALYSIS)
        chain = prompt | llm | StrOutputParser()

        inputs: GapAnalysisInput = {
            "job_description": "...",
            "work_experience": "..."
        }
        result = chain.invoke(inputs)
    """
    return load_prompt(name)
