"""Generator for type-safe get_prompt function with overloads."""

from __future__ import annotations

from pathlib import Path

from src.logging_config import logger

from .constants import PROMPTS_DIR
from .loader import load_prompt
from .names import PromptName


def generate_get_prompt() -> int:
    """Generate the get_prompt.py file with type-safe overloads.

    Scans the prompts directory, loads each prompt template to detect its runtime type,
    and generates a get_prompt() function with overloads that return the specific type.

    Returns:
        Number of overloads generated
    """
    # Scan prompts directory
    prompt_files = sorted(PROMPTS_DIR.glob("*.json"))

    if not prompt_files:
        logger.warning("No prompt files found in %s", PROMPTS_DIR)
        return 0

    # Collect data for each prompt template
    prompt_data: list[dict] = []
    type_names_seen: set[str] = set()

    for prompt_file in prompt_files:
        prompt_name_str = prompt_file.stem
        try:
            # Get the corresponding enum member
            prompt_name = PromptName(prompt_name_str)

            # Load the prompt template
            prompt = load_prompt(prompt_name)

            # Detect the runtime type
            prompt_type = type(prompt)
            type_name = prompt_type.__name__
            type_module = prompt_type.__module__

            # Validate type is recognized
            if type_name not in ("ChatPromptTemplate", "RunnableSequence"):
                raise ValueError(
                    f"Unrecognized prompt template type: {prompt_type} "
                    f"(module: {type_module}, name: {type_name})"
                )

            # Track unique types
            type_names_seen.add(type_name)

            prompt_data.append(
                {
                    "enum_name": prompt_name.name,
                    "type_name": type_name,
                    "type_module": type_module,
                }
            )

        except Exception as exc:
            logger.error(
                "Failed to process prompt %s: %s", prompt_name_str, exc, exception=True
            )
            continue

    if not prompt_data:
        logger.error("No prompt templates could be processed")
        return 0

    # Generate the file content
    content = _generate_get_prompt_file(prompt_data, type_names_seen)

    # Write to file
    output_file = Path("src/core/prompts/get_prompt.py")
    output_file.write_text(content, encoding="utf-8")

    logger.info("Generated get_prompt() with %d overloads", len(prompt_data))

    return len(prompt_data)


def _generate_get_prompt_file(prompt_data: list[dict], type_names_seen: set[str]) -> str:
    """Generate the complete get_prompt.py file content.

    Args:
        prompt_data: List of dicts with prompt metadata
        type_names_seen: Set of unique prompt template type names

    Returns:
        Complete file content as string
    """
    # Build imports based on detected types
    type_imports = []
    if "ChatPromptTemplate" in type_names_seen:
        type_imports.append("from langchain_core.prompts.chat import ChatPromptTemplate")
    if "RunnableSequence" in type_names_seen:
        type_imports.append("from langchain_core.runnables.base import RunnableSequence")

    type_imports_str = "\n".join(type_imports)

    # Build union type for implementation function
    if len(type_names_seen) == 1:
        union_type = list(type_names_seen)[0]
    else:
        union_type = " | ".join(sorted(type_names_seen))

    # Generate overloads
    overloads = []
    for data in prompt_data:
        overload = f'''@overload
def get_prompt(name: Literal[PromptName.{data["enum_name"]}]) -> {data["type_name"]}: ...
'''
        overloads.append(overload)

    overloads_str = "\n".join(overloads)

    # Assemble complete file
    content = f'''# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# Generated by: prompts sync
# To update: run `python cli.py prompts sync`

from __future__ import annotations

from typing import Literal, overload

{type_imports_str}

from .loader import load_prompt
from .names import PromptName


{overloads_str}

def get_prompt(name: PromptName) -> {union_type}:
    """Load a prompt template by name.

    This function provides type-safe prompt loading with overloads that return
    the specific concrete type for each prompt template.

    Args:
        name: The PromptName enum member

    Returns:
        The loaded prompt template (ChatPromptTemplate or RunnableSequence)

    Example:
        from src.core.prompts import PromptName, get_prompt
        from src.core.prompts.input_types import GapAnalysisInput

        prompt = get_prompt(PromptName.GAP_ANALYSIS)
        chain = prompt | llm | StrOutputParser()

        inputs: GapAnalysisInput = {{
            "job_description": "...",
            "work_experience": "..."
        }}
        result = chain.invoke(inputs)
    """
    return load_prompt(name)
'''

    return content

